#!/usr/bin/env python

import pygrib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
from netCDF4 import Dataset

date_from = '2015-05-15'
date_to = '2015-09-01'

path = '/scratch/leuven/336/vsc33651/nu-wrf-dev/Great_Slave_Lake/2015/postprd/'
filename_root = 'WRFPRS_'
domain = 'd01'
output_filename = os.path.join('/scratch/leuven/336/vsc33651/nu-wrf-dev/Great_Slave_Lake/2015/',filename_root+domain+'_2015.nc')

# determine length of time vector
ndays = (pd.to_datetime(date_to) - pd.to_datetime(date_from)).days
nhours = ndays * 24
hours = np.linspace(1,nhours,nhours)
# always with only one leading zero
length_counter = 2
cstr = "%0"+str(length_counter)+"d"
timestep = cstr % 1
filename = os.path.join(path,filename_root+domain+'.408')
gr=pygrib.open(filename)

lat = np.array(gr[500].values) #To get the actual values
lon = np.array(gr[501].values) #To get the actual values
lon = np.subtract(lon, 360) #To go from 0 - 360 to -180 - 180

ds = Dataset(output_filename, mode='w', format='NETCDF4')
timeunit = 'hours since 2000-01-01 00:00'
ds.createDimension('time', nhours)
ds.createDimension('lat', np.shape(lat)[0])
ds.createDimension('lon', np.shape(lon)[1])
ds.createVariable('time',hours.dtype, dimensions=('time',),zlib=True)
ds.variables['time'][:] = hours + (pd.to_datetime(date_from) - pd.to_datetime('2000-01-01')).days*24 - 1
ds.createVariable('lat',lat.dtype, dimensions=('lat','lon'),zlib=True)
ds.variables['lat'][:] = lat
ds.createVariable('lon',lon.dtype, dimensions=('lat','lon'),zlib=True)
ds.variables['lon'][:] = lon
ds.createVariable('PRATE_inst','int', dimensions=('time','lat','lon'),zlib=True)
ds.createVariable('PRATE_avg','f4', dimensions=('time','lat','lon'),zlib=True)
ds.createVariable('CAPE','f4', dimensions=('time','lat','lon'),zlib=True)
ds.createVariable('Cdim','f4', dimensions=('time','lat','lon'),zlib=True)
ds.createVariable('CPRAT','f4', dimensions=('time','lat','lon'),zlib=True)
ds.createVariable('Ctop','f4', dimensions=('time','lat','lon'),zlib=True)
ds.createVariable('Cbot','f4', dimensions=('time','lat','lon'),zlib=True)

ds.variables['time'].setncatts({'long_name': 'time', 'units': timeunit})
ds.variables['lat'].setncatts({'long_name': 'latitude',  'units': 'degrees_north'})
ds.variables['lon'].setncatts({'long_name': 'longitude', 'units': 'degrees_east'})
ds.variables['PRATE_avg'].setncatts({'long_name': 'Average precipitation rate',  'units': 'mm/s'})
ds.variables['PRATE_inst'].setncatts({'long_name': 'Instantaneous precipitation rate',  'units': 'mm/s'})
ds.variables['CAPE'].setncatts({'long_name': 'MIXED LAYER Convective available potential energy',  'units': 'J/kg'})
ds.variables['Cdim'].setncatts({'long_name': 'Vertical cloud dimensions', 'units': 'km'})
ds.variables['CPRAT'].setncatts({'long_name': 'Convective precipitation rate', 'units': 'kg/(m²s)'})
ds.variables['Cbot'].setncatts({'long_name': 'Vertical cloud dimensions', 'units': 'km'})
ds.variables['Ctop'].setncatts({'long_name': 'Vertical cloud dimensions', 'units': 'km'})

for h in range(408,nhours):
    ## Open the GRIB file
    timestep = cstr % h
    filename = os.path.join(path,filename_root+domain+'.'+timestep)
    print('processing '+filename)
    gr=pygrib.open(filename)
    #for g in gr:
    #    print(g.typeOfLevel, g.level, g.name)
    
    ## Extract the needed variables (latitude, longitude, cloud top and base height.
    #lat = gr.select(name='Latitude (-90 to +90)', typeOfLevel = 'surface') #To get the index
    #lon = gr.select(name='East Longitude (0 - 360)', typeOfLevel = 'surface') #To get the index
    #ctophgt = gr.select(name='Geopotential Height', typeOfLevel = 'cloudTop') #To get the index
    #ctoptemp = gr.select(name='Temperature', typeOfLevel = 'cloudTop') #Just in case it's needed
    #cbothgt = gr.select(name='Geopotential Height', typeOfLevel = 'cloudBase') #To get the index
    ctop = np.array(gr[468].values) #To get the actual values
    cbot = np.array(gr[466].values) #To get the actual values
    cdim = np.subtract(ctop, cbot) #To get the cloud vertical dimension
    cdim_km = np.divide(cdim, 1000) #To get the dimensions in km instead of m
    CPRAT = np.array(gr[450].values)
    CAPE = np.array(gr[495].values)
    PRATE_inst = np.array(gr[451].values)
    PRATE_avg = np.array(gr[452].values)

    ## Calculate the lightning flashes per minute according to the Price and Rind scheme (F = 3.44*10�~A�-5)*cdim_km(4.9)
    # catch negative height
    cdim_km[cdim_km<0.0] = 0.0
    # catch unrealistic high clouds, just in case
    # not sure whether it occurs, for individual hours I saw some pixels popping out with very high lightning, but still in range
    #cdim_km[cdim_km>18.3] = 18.3
    #power = np.power(cdim_km, 4.9)
    #F_per_min = np.multiply(power, 0.0000344)
    #print(F_per_min)
    #F_PH = np.multiply(F_per_min, 60) #To get hourly flashes (multiply by 60)

    #ds.variables['F_PH'][h,:,:] = F_PH
    ds.variables['CPRAT'][h, :, :] = CPRAT
    ds.variables['CAPE'][h, :, :] = CAPE
    ds.variables['Cdim'][h, :, :] = cdim_km
    ds.variables['PRATE_inst'][h, :, :] = PRATE_inst
    ds.variables['PRATE_avg'][h, :, :] = PRATE_avg
    ds.variables['Ctop'][h, :, :] = ctop
    ds.variables['Cbot'][h, :, :] = cbot

ds.close()
